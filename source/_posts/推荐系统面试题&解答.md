---
title: 推荐系统面试题&解答
date: 2021-12-15 14:22:35
tags: 推荐系统
categories: AI
cover: https://source.unsplash.com/m_HRfLhgABo/1200x628
---

# ML与DL基础

## 机器学习

<!--more-->

### 介绍下GBDT

  - gbdt 是通过采用加法模型（即基函数的线性组合），以及不断减小训练过程产生的残差来达到将数据分类或者回归的算法， gbdt通过多轮迭代,每轮迭代产生一个弱分类器，每个分类器在上一轮分类器的**残差**基础上进行训练。
  - GBDT中的树是回归树（不是分类树），默认选择CART回归树，GBDT用来做回归预测，调整后也可以用于分类。
  - 核心思想是**利用损失函数的负梯度在当前模型的值作为残差的近似值**，本质上是**对损失函数进行一阶泰勒展开**，从而拟合一个回归树。

 ### 介绍XGBoost

  - XGBoost是陈天奇开源的一种梯度提升树模型，是GBDT的一种工程实现。与GBDT最大的区别就是树的生成方式不同，加快了树的生成过程，以生成最优树。
  - XGBT相对于GBDT的优化（sklearn中的GBDT实现和传统的有一定改进，同样支持XGBT的一些特性，这里的对比只针对传统GBDT）：
    - 正则项：XGBT加入了正则项，控制模型复杂度，防止过拟合，加入的有叶子结点个数正则化、叶子结点输出L2正则化；
    - 二阶泰勒展开：XGBT对损失函数进行了二阶泰勒展开，加速收敛速度；
    - 支持更多基学习器：GBDT只支持CART树，XGBT支持多种基学习器，比如线性分类器；
    - 行采样：传统GBDT每一轮迭代都使用了全部数据，XGBT使用了行采样；
    - 列采样：传统GBDT同样没有使用列采样，XGBT引入了列采样；
    - 缺失值处理：GBDT没有缺失值处理机制，XGBT支持缺失值处理；
    - Shrinkage：对每一颗树输出进行衰减，削弱单颗树影响，让后续树有更大学习空间；
    - 并行化：特征粒度的并行化，而非树粒度的，在不同特征上采用多线程并行计算最优分割点；

 ### 介绍下LightGBM

  - LightGBM是微软开源的一个梯度Boosting框架，使用基于决策树的学习算法，是GBDT的一种工程实现，特点是快。
  - LGB相对于XGBT的改进：
    - 基于直方图的决策树算法：把特征离散到K个bin中构造直方图，遍历一遍特征统计直方图，最后根据直方图寻找最优分割点，这样做的好处是计算速度更快，内存占用更小；
    - 直方图做差加速：计算兄弟节点的直方图，只需要用父节点直方图-本节点直方图。这样做速度提升了一倍；
    - Leaf-wise叶子生长策略：XGBT的Level-wise每次分裂一层节点，容易并行化，但是更容易过拟合，Leaf-wise值分裂增益最大的节点，相对精度更高，过拟合更小；
    - 直接支持类别特征：第一个直接支持类别特征的GBDT工具，具体算法《On Grouping For Maximum Homogeneity》；
    - 高效并行优化：数据量小采用特征并行、数据并行，数据量大采用投票并行；
    - Cache优化：直方图算法天生提高缓存命中，降低内存消耗；
    - 单边梯度抽样算法：过滤梯度小的样本，同时平衡了数据分布的改变，这个算法能够提升计算速度；

 ### LightGBM相对于XGBoost的改进

  - 如上

 ### GBDT中的梯度是什么，怎么用

  - 在线性模型优化的过程中。利用**梯度下降**我们总是让参数**向负梯度的方向移动**，使损失函数最小。gbdt，假入我们现在有 t 课树，我们需要去学习是第 t+1 颗树，那么如何学习第 t+1 颗树才是最优的树呢？ 这个时候我们参考梯度优化的思想。现在的 t 课树就是我们现在的状态使用这个状态我们可以计算出现在的损失。如何让损失更小呢？我们只需要让 t+1 颗树去拟合损失的负梯度。而**残差** 是梯度在MSE为损失函数下的特例（MSE的导数就是残差）。 

 ### GBDT如何计算特征重要性

  - 树模型，天生能够利用Gini计算单颗树上特种重要性，然后再在所有树上汇总求平均；

 ### 介绍XGBoost中的并行

  - xgboost的**并行是在特征粒度**上的。我们知道，决策树的学习最耗时的一个步骤就是**对特征的值进行排序**（因为要确定最佳分割点），xgboost在训练之前，预先对数据进行了排序，然后保存为block结构，后面的迭代中重复地使用这个结构，大大减小计算量。这个block结构也使得并行成为了可能，在进行节点的分裂时，需要计算每个特征的增益，最终选增益最大的那个特征去做分裂，那么**各个特征的增益计算就可以开多线程进行**。

 ### 介绍XGBoost中精确算法与近似算法

  - 指的是在计算特征分裂的时候，XGBT使用了近似算法
  - 精确算法：通过列举所有特征的可能划分找到最优划分解来生成树，该方法**需要排序以形成连续的特征，之后计算每个可能的梯度统计值**。
    - 缺点：在数据量非常大的情况下，精确基本用不了。一方面在生成树的过程中，每次寻找一个节点最佳分割点时，都需要比较其在所有特征的**所有分割点**上的效果，这么做时间复杂度很高；另一方面，在每次对某个特征进行分割的时候，需要对所有样本根据该特征值进行排序，这需要我们把所有的数据存储在内存中，这会给硬件方面带来很大压力。

  - 近似算法：在针对一个特征寻找分割点的时候，我们其实对特征中的值的范围不敏感，只对这些值的顺序敏感。比如数据集中的样本的某一个特征出现的值有12、15、82、107，但是如果我们把这四种值分别替换成1、2、3、4，最后得到的树的结构是不变的。利用这种思想，给出一个数据集中样本的第k个特征和样本点在损失函数上的二阶导数所组成的集合，随后利用数据分布的百分比来定义一个排名函数 ，这个排名函数代表了特征k的值小于z的样本占总样本的比例。我们的目标就是利用这个排名函数来寻找候选分割点集合。

 ### XGBoost如何处理空缺值

  - 将缺失值分别划分到左子树和右子树，分别计算出左子树和右子树的增益 ，选出更大的，将该方向作为缺失值的分裂方向（记录下来，预测阶段将会使用）。
  - LGB使用相同的方法；

 ### 为何要进行行采样、列采样

  - 简单回答：降低了过拟合
  - 具体回答：（数学证明）

 ### 为什么高维稀疏数据，LR比GBDT要好

  - 树模型对稀疏特征，切分的收益非常小，只能从少量非0信息上学习；
  - 线性模型的正则项是对权重惩罚，树模型是对深度、叶子个数的惩罚。所以高维稀疏数据中，少量样本会对结果产生非常大的影响，非常容易过拟合，而线性模型的权重惩罚能够很好处理这一点。综上，带正则化的线性模型比较不容易对稀疏特征过拟合；
  - 同样的原因可以解释为什么onehot不适合树模型；

 ### 随机森林与GBDT采样的区别

  - RF采用了行列采样，传统GBDT算法没有采用；

 ### 随机森林中列采样的作用

  - 随机森林在bagging基础上，进一步在训练过程引入随机属性选择，从全集d中随机选择k个属性的子集，利用这个子集来建立本颗子树，下一轮同理；推荐的k=log2d

 ### bagging与boosting对比

  - boosting：串行的方式训练基分类器，各分类器之间有依赖。每次训练时，对前一层基分类器分错的样本给与更高的权重，更多的关注的是偏差；

  - bagging：是Bootstrap aggregating的意思，各分类器之间无强依赖，可以并行，最终结果进行投票（分类），或者平均（回归）；

  - 样本选择上：

    - Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。

    - Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。

  - 样例权重：

    - Bagging：使用均匀取样，每个样例的权重相等。

    - Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

  - 预测函数：

    - Bagging：所有预测函数的权重相等。

    - Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

  - 并行计算：

    - Bagging：各个预测函数可以并行生成。

    - Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

 ### bagging与boosting分别从什么角度降低过拟合

  - bagging降低方差，boosting降低方差

 ### 逻辑回归如何避免过拟合

  - 更多数据集、数据增强、更多特征；
  - 权重衰减正则化；
  - 提前终止；

 ### 推导逻辑回归损失函数和损失函数求导



 ### 正则化项L1和L2为什么有用

  - L1正则化和L2正则化可以看做是损失函数的惩罚项。所谓『惩罚』是指对损失函数中的某些参数做一些限制；
  - 拟合过程中通常都倾向于让权值尽可能小，最后构造一个所有参数都比较小的模型。因为一般认为参数值小的模型比较简单，能适应不同的数据集，也在一定程度上避免了过拟合现象。

 ### l1正则不可导，如何优化

  - 在不可导处无法进行梯度下降，此时采用坐标轴下降法：坐标轴下降法是沿着坐标轴的方向，每次固定m-1个数值，对最后一个数值求局部最优解，迭代m次（证明：凸函数在每一个维度都取得最小值，则此处就是全局最小值）；
  - 同样可以用Proximal operator、admm等方法；

 ### 什么样的特征容易产生比较小的权重

  - ？

 ### 随机森林采样n次，n趋于无穷大，oob样本的概率接近于？

  - 1/e：limx->∞（1-1/x)^x

 ### 逻辑回归与树模型的优缺点

  - 树模型
    - 可解释性强，比线性模型还强
    - 拟合能力更强，特别是对非线性数据；
    - 容易过拟合；

 ### 对于高维稀疏数据，树模型能训练吗？一般怎么处理

  - 能训练，但是效果不好，容易过拟合；
  - 处理方法是：？

 ### 树模型一般有哪些参数，分别有什么作用

  - num_leaves: 最大叶子节点个数
  - max_depth
  - learning_rate
  - min_split_gain

 ### 随机森林如何处理空缺值

  - 随机森林本身没有处理空缺值算法，有些实现中附带了处理空缺值算法；
  - 数值变量用中位数、类别变量用众数；
  - 利用无空缺的变量计算相似度后加权计算，类别变量用加权投票，数值变量加权平均；

 ### 介绍kmeans，与其他聚类算法的对比

  - K-means 是我们最常用的基于欧式距离的聚类算法，其认为两个目标的距离越近，相似度越大；
  - 所以 K-means 的算法步骤为：
    1. 选择初始化的 k 个样本作为初始聚类中心 ![[公式]](https://www.zhihu.com/equation?tex=a%3D%7Ba_1%2Ca_2%2C%E2%80%A6a_k%7D) ；
    2. 针对数据集中每个样本 ![[公式]](https://www.zhihu.com/equation?tex=x_i) 计算它到 k 个聚类中心的距离并将其分到距离最小的聚类中心所对应的类中；
    3. 针对每个类别 ![[公式]](https://www.zhihu.com/equation?tex=a_j) ，重新计算它的聚类中心 ![[公式]](https://www.zhihu.com/equation?tex=a_j%3D%5Cfrac%7B1%7D%7B%5Cleft%7C+c_i+%5Cright%7C%7D%5Csum_%7Bx%5Cin+c_i%7Dx) （即属于该类的所有样本的质心）；
    4. 重复上面 2 3 两步操作，直到达到某个中止条件（迭代次数、最小误差变化等）。
  - 优点：
    - 容易理解，聚类效果不错，虽然是局部最优， 但往往局部最优就够了；
    - 处理大数据集的时候，该算法可以保证较好的伸缩性；
    - 当簇近似高斯分布的时候，效果非常不错；
    - 算法复杂度低。

  - 缺点：
    - K 值需要人为设定，不同 K 值得到的结果不一样；
    - 对初始的簇中心敏感，不同选取方式会得到不同结果；
    - 对异常值敏感；
    - 样本只能归为一类，不适合多分类任务；
    - 不适合太离散的分类、样本类别不平衡的分类、非凸形状的分类。

 ### 机器学习导致误差的原因？

  - 偏差：模型无法表达数据集的复杂度，模型不够复杂，导致不能学习到基本关系，导致欠拟合；
  - 方差：数据量有限，模型对数据过度敏感，导致方差；

 ### 过拟合、欠拟合对应的偏差和方差是怎样的？

  - 过拟合：高方差，低偏差
  - 欠拟合：高方差、高偏差

 ### 如何解决过拟合问题？哪些角度

  - 更多数据、数据增强；
  - 更换模型：更简单模型、更优化的模型；
  - 权重衰减正则化；
  - bagging等集成学习方法，深度学习中的dropout；
  - early stopping；

## 深度学习

 ### 优化器，SGD与Adam的异同点
  - SGD有两大改进方向：动量上改进、自适应学习率改进
  - Adam同时结合了这两者的改进方法：在动量上用了Momentum，自适应学习率上用了RMSprop

 ### SGD缺点，已经有什么改进的优化器
  - 每次只使用一批样本，导致迭代方向变化很大，容易剧烈震荡；
  - 学习率固定，容易在局部下降速度过慢过过快，得到局部最优解或者学习过慢；
  - 改进方法就是动量和自适应学习率：momentum、adagrad、Adam、等；

 ### 网络权重初始化为0有什么影响，初始化为一个非0的常数呢？
  - 如果W、b初始化为0：每一层前向传播输出都是一致的，反向传播同样就一致，多个神经元作用等同于1个；
  - 只有W初始化为0：b随机初始化：反向传播过程中，第一次的第一层的dw都是0，只有第二次才能恢复，导致收敛更慢，梯度消失问题严重；
  - 只有b初始化为0：可以的

 ### embedding如何设置维度？越大越好还是越小越好？
  - 维度越低越粗糙，拟合能力就有限；
  - 阅读越高越细致，但是需要更多数据集才能训练，容易有维度灾难；
  - 具体大小需要结合实际数据集大小，问题规模，经验参数，以及调试得到；

 ### transformer中计算attention除于根号d的作用
 ### embedding如何训练
  - CBOW: 先在句子中选定一个中心词，并把其它词作为这个中心词的上下文。在学习过程中，使用上下文的词向量推理中心词，这样中心词的语义就被传递到上下文的词向量中, 从而达到学习语义信息的目的。
  - Skip-gram: 同样先选定一个中心词，并把其他词作为这个中心词的上下文。不同的是，在学习过程中，使用中心词的词向量去推理上下文，这样上下文定义的语义被传入中心词的表示中， 从而达到学习语义信息的目的。
  - 一般来说，CBOW比Skip-gram训练速度快，训练过程更加稳定，原因是CBOW使用上下文average的方式进行训练，每个训练step会见到更多样本。而在生僻字（出现频率低的字）处理上，skip-gram比CBOW效果更好，原因是skip-gram不会刻意回避生僻字(CBOW结构中输入中存在生僻字时，生僻字会被其它非生僻字的权重冲淡)

 ### 介绍下attention，相比cnn、lstm的优势
 ### word2vec如何进行负采样
  - 负采样的核心思想是：就是分别计算正负样本的loss，这样负样本就可以选择采样的那几条，而不是除开正样本以外的所有样本。
  - 一个单词被选作negative sample的概率跟它出现的频次有关，出现频次越高的单词越容易被选作negative words

 ### word2vec两种训练方法的区别，具体损失函数
  - 如上

 ### 介绍LSTM每一个门的具体操作，一个LSTM cell的时间复杂度是多少
  - forget gate：决定上一时刻的单元c-1有多少保存到当前时刻c
  - input gate：决定当前时刻的输入x，有多少保存下来到c
  - output gate：决定当前单元c，有多少输出的当前的输出值h
  - 复杂度没有找到相关资料；

 ### transformer中encoder和decoder的输入分别是什么
 ### transformer中encoder与decoder的QKV矩阵如何产生
 ### transformer中QKV矩阵是否可以设置成同一个
 ### transformer与bert的位置编码有什么区别
 ### BERT中计算attention的公式
 ### BERT中LayerNorm的作用，为什么不用BN？
 ### BERT中的两种预训练任务介绍
 ### 深度学习中BN的好处？最早提出BN是为了解决什么问题？BN具体怎么实现的
  - 随着训练进行，数据的分布会发生变化，会导致训练困难。如果没有 BN 层，深度神经网络中的每一层的输入数据或大或小、分布情况等都是不可控的。有了 BN 层之后，每层的数据分布都被转换在均值为零，方差为1 的状态，这样每层数据的分布大致是一样的，训练会比较容易收敛。
  - 神经网络在训练时比较容易收敛，更容易避免梯度消失、梯度爆炸；

 ### 激活函数中，sigmoid，tanh有什么不好的地方？relu有什么优势？
  - sigmoid、tanh缺点
    - 两端有梯度消失；
    - 有指数等运算，求导更复杂；

  - relu优点：
    - 求导快，梯度固定；
    - 避免了两端梯度消失现象；

## 特征工程

 ### 特征工程一般怎么做
  - 特征分类
  - 特征预处理
  - 特征构建
  - 特征选择
  - 特征评估
 ### 特征数值分布比较稀疏如何处理
  - embedding
  - 用对稀疏特征优化更好的算法，比如FM等；

 ### 正负样本不均衡如何处理
  - 采样（欠采样、过采样
  - 集成学习
  - 对较少样本分类错误增加更高惩罚

 ### 连续特征离散化的作用
  - 增强模型鲁棒性，减少噪声的影响，减少过拟合
  - 增强表达能力，引入了非线性表达，减少偏差
  - 模型运算速度更快，储存所用空间更少

 ### 对id类特征onehot导致维度过高，如何处理？
  - embedding

 ### 如何进行特征筛选
  - 过滤法：按照相关性等指标对特征评分，进行特征选择
  - 包装法：每次选择部分特征进行训练
  - 嵌入法：使用能够计算特征重要性的模型（比如树、线性模型），找到最重要的特征

## 评估指标

 ### auc的含义和计算方法
  - **[ROC曲线](https://baike.baidu.com/item/ROC曲线/775606)**全称为[受试者工作特征曲线](https://baike.baidu.com/item/受试者工作特征曲线/12718177) （receiver operating characteristic curve），它是根据一系列不同的二分类方式（分界值或决定阈），以真阳性率（敏感性）为纵坐标，假阳性率（1-特异性）为横坐标绘制的**[曲线](https://baike.baidu.com/item/曲线/12004395)**。
  - **AUC**（Area Under Curve）被定义为ROC曲线下的面积
  - auc两种绘制方法：
    - 绘制ROC，分段计算面积；
    - 利用auc另一种解释：正样本在负样本前的概率。具体如下：假设有m个正样本、n个负样本，共有mn个样本对，计算其中正样本在负样本前的概率；

 ### 如果对负样本进行采样，auc的计算结果会发生变化吗
  - 针对负样本做随机采样，或者针对正样本做随机采样，或者全局做随机采样，保证随机采样后正负样本分布不变，这个时候auc对采样不敏感。

 ### 交叉熵跟MSE有什么区别
  - 一个用于分类任务，一个用于回归任务；
  - MSE是假设数据符合高斯分布时,模型概率分布的负条件对数似然;
  - 交叉熵是假设模型分布为多项式分布时,模型分布的负条件对数似然；
  - MSE无差别得关注全部类别上预测概率和真实概率的差；
  - 交叉熵关注的是正确类别的预测概率；

 ### micro-f1解释
  - micro f1不需要区分类别，直接使用总体样本的准召计算f1 score；
  - 在推荐系统中，种类中数量较多的商品会对f1造成更大的影响力；
  - Macro F1分类别计算精确率和召回率，求均值后计算f1；

 ### 介绍下排序指标ndcg
  - 归一化折损累计增益，NDCG用作排序结果的评价指标，这个指标通常是用来衡量和评价搜索结果算法；
  - ndcg@n 只关心前n个排序是否正确，后面的排序正不正确不予考虑。ndcg@n 的计算方式比较特别，要进行两次排序，一次是对预测的结果排序，另一次是对实际的分布排序；



# 推荐模型相关

## 召回

 ### 介绍双塔模型
  - 双塔模型最大的特点就是**「user和item是独立的两个子网络」**，左侧是用户塔，右侧是item塔，这两个塔的参数不共享；
  - 输入层：**「[User特征]」**主要包括和用户相关的特征：用户id、手机系统、地域、年龄、历史行为序列等，**「Item特征」**主要包括和Item相关的特征：ItemId、Item类别、Item来源等；
  - 表示层：User特征和Item特征分别输入到特征提取网络（比如说DNN等）得到User Embedding和Item Embedding。之后我们可以计算这两个Embedding之间的余弦距离。**「用户点击过的Item其距离更近，用户没有点击过或者讨厌的Item其距离更远」**。之后利用算得的loss来更新模型的参数。
  - 匹配层：拿用户向量去FAISS中和Item向量进行相似度计算，并返回距离最近的Top K个Item作为个性化的召回结果。

 ### 双塔模型的输出，用双塔embedding做内积+sigmoid和求余弦相似度+sigmoid的区别
 ### 双塔模型一般怎么做特征
  - 每个塔各自构建user与item embedding，**[User特征]」**主要包括和用户相关的特征：用户id、手机系统、地域、年龄、历史行为序列等，上下文特征（Context feature）可以放入用户侧塔，**「Item特征」**主要包括和Item相关的特征：ItemId、Item类别、Item来源等；

 ### 双塔模型为什么不直接把两个塔合起来输入一个DNN
  - 性能：减少线上运算速度，item塔可以提前训练，线上只需要user的embedding和相似度计算；

## 排序

 ### 为什么CTR中目前普遍使用深度学习模型替换树模型？
  - 强大的表达能力，能够挖掘更深层次数据模式；
  - 模型结构非常灵活，能够根据实际应用场景进行调整
 ### 为什么要有wide层、FM层，deep层不也有记忆能力吗
  - wide层记忆能力更强，因为它结构简单，原始数据能够直接影响推荐结果，能够学习到数据中的简单规则
 ### DeepFM与wide&deep的介绍与对比
  - Wide&Deep模型同时考虑了记忆能力和泛化能力，但Wide部分需要人工参与特征工程；DeepFM对Wide&Deep模型的改进之处在于用FM替换了原来的Wide部分,加强了浅层网络部分特征组合的能力。
  - DeepFM的动机非常直观，既希望考虑高/低阶的feature interaction，又想省去额外的特征工程。使用FM取代Wide的LR部分是一个可行的做法，当然这里LR可以基于先验构造更高阶的组合特征，而FM只考虑二阶，DeepFM中的FM层和隐藏层共享输入，这种共享输入使得DeepFM可以同时从原始特征中学习低阶特征交互和高阶特征交互,完全不需要执行特征工程。
 ### 对DeepFM进行优化，有哪些思路
 ### DeepFM如果过拟合和欠拟合分别如何处理
 ### 介绍除了FM之外的特征交叉的模型
  - FNN：有高阶bit-wise特征交叉，每个特征都使用了与训练的FM模型，训练开销更低。
  - DeepFM：是一种可以从原始特征中抽取到各种复杂度特征的端到端模型，没有人工[特征工程](https://www.zhihu.com/search?q=特征工程&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A269730650})的困扰，DeepFM模型包含FM和DNN两部分，FM模型可以抽取low-order特征，DNN可以抽取high-order特征。无需类似Wide&Deep模型人工特征工程。
  - DCN：可以任意组合特征，而且不增加网络参数.Cross的目的是以一种显示、可控且高效的方式，自动构造有限高阶交叉特征。


 ### 介绍DIN模型，适合的场景


  - 在DIN出现之前，推荐系统或者广告系统的做法通常是将高维的稀疏输入通过一个embedding层转化为低维稠密的特征表示，之后将同类的embedding特征通过pooling的方式（sum pooling或者avg pooling）转化为固定长度的特征，最后将不同类特征拼接起来输入到网络中进行训练。
  - DIN模型，增加了注意力机制，D模型的创新点或者解决的问题就是使用了注意力机制来对用户的兴趣动态模拟， 而这个模拟过程存在的前提就是用户之前有大量的历史行为了，这样我们在预测某个商品广告用户是否点击的时候，就可以参考他之前购买过或者查看过的商品，这样就能猜测出用户的大致兴趣来，这样我们的推荐才能做的更加到位，所以这个模型的使用场景是**非常注重用户的历史行为特征（历史购买过的商品或者类别信息）**

 ### DIN中如何计算attention


  - 计算attention的方式是利用用户行为的Embedding向量和广告的Embedding向量来进行计算，具体文章中采用的是用一个神经网络（activation unit）来得到weight；

  - activation unit的输入包括两个部分，一个是原始的用户行为embedding向量、广告embedding向量；另外一个是两者Embedding向量经过外积计算后得到的向量，文章指出这种方式有利于relevance modeling。 

    除此之外文章用到的Attention机制不是原始NLP任务中采用的Attention，文章中放宽了对于权重加和等于一的限制，这样更有利于体现不同用户行为特征之间的差异化程度。

 ### transformer与DIN的区别和联系

 ### 介绍下listwise排序模型LambdaRank


  - Listwise方法是直接优化排序列表，输入为单条样本为一个**文档排列**。通过构造合适的度量函数衡量当前文档排序和最优排序差值，优化度量函数得到排序模型。



# 热门技术相关

## Embedding

 ### 介绍下item2vec模型
  - 相比于Word2vec利用“词序列”生成词Embedding。Item2vec利用“物品序列”构造物品Embedding。 其中物品序列是由指定用户的浏览购买等行为产生的历史行为序列；
  - 利用用户行为序列，采用word2vec思想，生成每个item的Embedding，同样user embedding：由历史item embedding平均或聚类得到；

 ### embedding冷启动怎么做
  - 补充side information：加入一些其他类型的特征，典型的用户侧特征是人口统计特征，典型的物品侧特征是一些内容型特征；
  - 从推荐架构改进：批处理->流处理->实时推断->边缘计算，让新信号的消费变得越来越实时；
  - 冷启动机制：比如采用Airbnb方案，利用物品之间的相似性，对冷启动物品根据相似物品，快速生成初始化Embedding。或者聚类，决策树等经典模型；



## 多任务学习

 ### 多任务学习模型的发展历史详细介绍

 ### 为什么要用多任务学习

 ### 介绍MMOE、PLE、ESMM，PLE相对MMOE的改进

 ### ESSM算法原理和解决的两个问题

 ### ESMM中如何解决CVR样本过于稀疏的问题，实际上解决了吗

 ### ESMM训练是否使用全量样本

 ### 介绍PLE模型

 ### PLE里面loss如何平衡

 ### PLE模型中，是否有尝试对不同的gate用不同的特征，是否有尝试不同业务用不同的特征组合

 ### gradnorm介绍

 ### 介绍关于多任务权重设置的相关模型或者策略

 ### 如何平衡不同任务的loss

 ### 如果一个特征对任务a是正相关，对任务b是负相关，如何处理这个特征

 ### CTR和CVR任务放在ESMM（都是曝光空间）里和放在PLE（CTR点击空间，CVR曝光空间）里哪种效果好 

## 模型蒸馏

 ### 介绍下蒸馏的loss

## 图神经网络

### GCN、GraphSAGE、GAT的区别与联系
### node2vec对比deepwalk的改进
  - node2vec的思想同DeepWalk一样，生成随机游走，对随机游走采样得到（节点，上下文）的组合，然后用处理词向量的方法对这样的组合建模得到网络节点的表示。不过在生成随机游走过程中做了一些创新，node2vec改进了DeepWalk中随机游走的生成方式(通过调整随机游走权重的方法使graph embedding的结果在网络的同质性（homophily）和结构性（structural equivalence）中进行权衡)，使得生成的随机游走可以反映深度优先和广度优先两种采样的特性，从而提高网络嵌入的效果。

### graphsage对比gcn的优势
### transductive与Inductive的区别
### 训练图模型的loss有哪些
### graph embedding的作用

# 业务场景相关

 ### CVR相比CTR的区别、特点

 ### 搜索与推荐的区别，你认为哪个难度更大

  - 排得更好VS估得更准VS搜的更全「推荐、广告、搜索」算法间到底有什么区别？ - 王喆的文章 - 知乎 https://zhuanlan.zhihu.com/p/430431149

 ### 广告与推荐的区别

  - 排得更好VS估得更准VS搜的更全「推荐、广告、搜索」算法间到底有什么区别？ - 王喆的文章 - 知乎 https://zhuanlan.zhihu.com/p/430431149

 ### 什么时候用规则，什么时候用模型

 ### 线上如何生成最终的排序得分

 ### 正负样本介绍（规模和比例），如何构造负样本

  - 正样本可以定义为用户当天播放过的节目，也就是“喜欢”；

  - 负样本则有两种选择方案：

    （1）负样本指的是对用户曝光过的节目，但是用户至始至终都没有播放过，也就是说该节目并不在“历史”和“喜欢”两个分类里面；
    （2）负样本指的是在整个抽样的池子里面，但是用户至始至终都没有播放过，也就是说该节目并不在“历史”和“喜欢”这两个分类里面，**我们会选择给用户曝光但是用户没有进行播放的节目作为负阳本**；

  - 假设正样本的条数是N，则负样本的条数会控制在2N或者3N，即遵循1:2或者1:3的关系，当然具体的业务场景下要进行不同的尝试和离线评估指标的对比。

 ### 低活用户特征稀疏如何解决

  - 扩充用户画像：注册时统计更多用户静态信息，询问用户喜欢类别等；

